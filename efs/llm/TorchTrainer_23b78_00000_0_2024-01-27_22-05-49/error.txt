Failure # 1 (occurred at 2024-01-27_22-06-45)
[36mray::_Inner.train()[39m (pid=36524, ip=192.168.1.209, actor_id=8fda9f9a69f2c0a9d16f15ef01000000, repr=TorchTrainer)
  File "/home/ppundir/Desktop/Desktop/mahcine-learning-operations/mlops/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/home/ppundir/Desktop/Desktop/mahcine-learning-operations/mlops/lib/python3.10/site-packages/ray/train/_internal/utils.py", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.209, ID: ac4d04c95321f0923265889d6d37a2f3ad8acb3539aba521d5427579) where the task (actor ID: 53cf19b1e632e874abc91e0801000000, name=RayTrainWorker.__init__, pid=36609, memory used=0.80GB) was running was 12.93GB / 13.59GB (0.95145), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 987b66d0102bb905c7213648e2b791e52fda703d486f122f1dac57b5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.209`. To see the logs of the worker, use `ray logs worker-987b66d0102bb905c7213648e2b791e52fda703d486f122f1dac57b5*out -ip 192.168.1.209. Top 10 memory users:
PID	MEM(GB)	COMMAND
36609	0.80	ray::_RayTrainWorker__execute.get_next
36606	0.72	ray::_RayTrainWorker__execute.get_next
36611	0.72	ray::_RayTrainWorker__execute.get_next
36608	0.69	ray::_RayTrainWorker__execute.get_next
36610	0.69	ray::_RayTrainWorker__execute.get_next
36607	0.36	ray::_RayTrainWorker__execute.get_next
8776	0.17	/usr/share/code/code --type=renderer --crashpad-handler-pid=8676 --enable-crash-reporter=6f230fa1-d6...
10163	0.16	/usr/lib/firefox/firefox-bin -contentproc -childID 30 -isForBrowser -prefsLen 32320 -prefMapSize 236...
8711	0.16	/usr/share/code/code --type=renderer --crashpad-handler-pid=8676 --enable-crash-reporter=6f230fa1-d6...
35611	0.12	/home/ppundir/Desktop/Desktop/mahcine-learning-operations/mlops/bin/python -m ipykernel_launcher --f...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
